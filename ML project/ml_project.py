# -*- coding: utf-8 -*-
"""ML project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sl-GeRMNG-XakNVB4PYzTbaNtUplXFJA
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
pd.set_option('display.max_colwidth', None)
from google.colab import files
uploaded = files.upload()

df = pd.read_csv("IMDB Dataset.csv", encoding='utf-8', on_bad_lines='skip')
print(df.columns)

df = pd.read_csv("IMDB Dataset.csv", encoding='utf-8', sep=';', on_bad_lines='skip')
df.head()

df[['review', 'sentiment']] = df['review,sentiment'].str.rsplit(',', n=1, expand=True)

# حذف العمود القديم
df.drop(columns=['review,sentiment'], inplace=True)

# إعادة ضبط الفهرسة
df.reset_index(drop=True, inplace=True)

# عرض أول 5 صفوف للتأكد
df.head()

print(df.columns)

df.shape

df.head()
df.info()
df.isnull().sum()
df['sentiment'].value_counts()
print(df['review'].iloc[123])
df.dropna(subset=['sentiment'], inplace=True)
df.reset_index(drop=True, inplace=True)

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

# عمل نسخة من البيانات الأصلية
df_clean = df.copy()

# أدوات التنظيف
stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))

# دالة التنظيف
def clean_text(text):
    text = re.sub('<.*?>', '', text)  # إزالة HTML tags
    text = re.sub('[^a-zA-Z]', ' ', text)  # حذف الرموز والأرقام
    text = text.lower()  # تحويل للحروف الصغيرة
    words = text.split()
    words = [stemmer.stem(word) for word in words if word not in stop_words]
    return ' '.join(words)

# تطبيق الدالة على عمود review
df_clean['review'] = df_clean['review'].apply(clean_text)

# عرض أول 5 مراجعات بعد التنظيف
df_clean.head()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# تحويل النصوص إلى تمثيل TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)  # نختار 5000 ميزة كحد أقصى
X = vectorizer.fit_transform(df_clean['review']).toarray()
y = df_clean['sentiment']

# تقسيم البيانات (80% تدريب - 20% اختبار)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# تحويل النصوص إلى أرقام
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df_clean['review']).toarray()
y = df_clean['sentiment'].map({'positive': 1, 'negative': 0})  # تحويل إلى أرقام

# تقسيم البيانات
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_lr))

from sklearn.naive_bayes import MultinomialNB

nb = MultinomialNB()
nb.fit(X_train, y_train)
y_pred_nb = nb.predict(X_test)

print("Naive Bayes Accuracy:", accuracy_score(y_test, y_pred_nb))

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)

print("KNN Accuracy:", accuracy_score(y_test, y_pred_knn))

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=50)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)

print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dt))

from sklearn.neural_network import MLPClassifier

ann = MLPClassifier(hidden_layer_sizes=(32,), max_iter=100, solver='adam', early_stopping=True)
ann.fit(X_train, y_train)
y_pred_ann = ann.predict(X_test)

print("ANN Accuracy:", accuracy_score(y_test, y_pred_ann))

import pandas as pd
import os

os.makedirs("preprocessed data", exist_ok=True)

# حفظ الملفات المطلوبة
pd.DataFrame(X_train).to_csv("preprocessed data/X.csv", index=False)
pd.DataFrame(X_test).to_csv("preprocessed data/X_test.csv", index=False)
pd.DataFrame(y_train).to_csv("preprocessed data/Y.csv", index=False)
pd.DataFrame(y_test).to_csv("preprocessed data/Y_test.csv", index=False)

# إنشاء مجلد Results
os.makedirs("Results", exist_ok=True)

# توقعات Random Forest
pred_rf = rf.predict(X_test)
pd.DataFrame(pred_rf).to_csv("Results/predictions_RF_model.csv", index=False)

# توقعات Naive Bayes
pred_nb = nb.predict(X_test)
pd.DataFrame(pred_nb).to_csv("Results/predictions_NB_model.csv", index=False)

# توقعات ANN
pred_ann = ann.predict(X_test)
pd.DataFrame(pred_ann).to_csv("Results/predictions_ANN_model.csv", index=False)

# توقعات Logistic Regression
pred_lr = lr.predict(X_test)
pd.DataFrame(pred_lr).to_csv("Results/predictions_LR_model.csv", index=False)

# توقعات KNN
pred_knn = knn.predict(X_test)
pd.DataFrame(pred_knn).to_csv("Results/predictions_KNN_model.csv", index=False)

# توقعات Decision Tree
pred_dt = dt.predict(X_test)
pd.DataFrame(pred_dt).to_csv("Results/predictions_DT_model.csv", index=False)

import os
import pandas as pd

os.makedirs("original data", exist_ok=True)

try:
    # قراءة الملف الأصلي بصيغة آمنة
    df_original = pd.read_csv("IMDB Dataset.csv", encoding='ISO-8859-1', quotechar='"', on_bad_lines='skip')
    df_original.to_csv("original data/IMDB Dataset.csv", index=False)
    print("تم حفظ النسخة الأصلية من IMDB Dataset بنجاح.")
except:
    # fallback إذا الملف الأصلي فشل
    df_clean.to_csv("original data/IMDB Dataset.csv", index=False)
    print("فشل في قراءة النسخة الأصلية، تم حفظ df_clean بدلاً منها.")